{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7838eb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "from torch.nn import LayerNorm,attention,Embedding\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a627677a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 76052 | Val: 7069 | Test: 6740\n"
     ]
    }
   ],
   "source": [
    "def load_pairs_from_file(file_path):\n",
    "    pairs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for i in range(0, len(lines), 3):  # every 3 lines: input, response, blank\n",
    "            if i + 1 < len(lines):\n",
    "                input_line = lines[i].strip().replace(\"Input: \", \"\")\n",
    "                response_line = lines[i + 1].strip().replace(\"Response: \", \"\")\n",
    "                pairs.append((input_line, response_line))\n",
    "    return pairs\n",
    "\n",
    "# Load datasets\n",
    "train_pairs = load_pairs_from_file(\"dataset/conver_train.txt\")\n",
    "val_pairs   = load_pairs_from_file(\"dataset/conver_val.txt\")\n",
    "test_pairs  = load_pairs_from_file(\"dataset/conver_test.txt\")\n",
    "\n",
    "print(f\"Train: {len(train_pairs)} | Val: {len(val_pairs)} | Test: {len(test_pairs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd79476",
   "metadata": {},
   "source": [
    "## Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90407aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.strip().split()\n",
    "\n",
    "\n",
    "def build_vocab (pairs,min_frequency = 2):\n",
    "    counter  = Counter()\n",
    "    for input, resp in pairs:\n",
    "        counter.update(tokenize(input))\n",
    "        counter.update(tokenize(resp))\n",
    "    vocab_words = [word for word, freq in counter.items() if freq >= min_frequency]\n",
    "    vocab = SPECIAL_TOKENS + sorted(vocab_words)\n",
    "    \n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5961c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 25191\n"
     ]
    }
   ],
   "source": [
    "word2idx, idx2word = build_vocab(train_pairs, min_frequency=1)\n",
    "\n",
    "print(f\"Vocab size: {len(word2idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ec6f9d",
   "metadata": {},
   "source": [
    "## Load Fasttext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c9f5fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = KeyedVectors.load_word2vec_format('fasttext file/cc.en.300.vec', binary=False)\n",
    "# https://fasttext.cc/docs/en/crawl-vectors.html choose English choose text .vec file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95f51213",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((len(word2idx),embedding_dim))\n",
    "\n",
    "for words,idx in word2idx.items():\n",
    "    if words in fasttext_model:\n",
    "        embedding_matrix[idx] = fasttext_model[words]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale= 0.6, size = (embedding_dim) )\n",
    "\n",
    "fasttext_embeddings = torch.tensor(embedding_matrix, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a217098",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2c99efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoded_sentences(sentence,word2idx):\n",
    "    token =  sentence.strip().split()\n",
    "    return  [ word2idx.get(word,word2idx['<unk>']) for word in token]\n",
    "\n",
    "def encoded_pair(pair,word2idx):\n",
    "    encoded = []\n",
    "    for input,resp in pair:\n",
    "        inp_ids = encoded_sentences(input,word2idx)\n",
    "        res_ids = [word2idx['<sos>']] + encoded_sentences (resp,word2idx) + [word2idx['<eos>']]\n",
    "        encoded.append((inp_ids,res_ids))\n",
    "    return encoded\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f056e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [5909, 17, 3905, 17, 14859, 7487, 13949, 13461, 7469, 13114, 8715, 7736, 11654, 1067]\n",
      "Response: [1, 7403, 15813, 23000, 15482, 22911, 9432, 15482, 19716, 17607, 13959, 13461, 18023, 13252, 20, 2]\n"
     ]
    }
   ],
   "source": [
    "encoded_train = encoded_pair(train_pairs, word2idx)\n",
    "encoded_val   = encoded_pair(val_pairs, word2idx)\n",
    "encoded_test  = encoded_pair(test_pairs, word2idx)\n",
    "\n",
    "print(\"Input:\", encoded_train[0][0])\n",
    "print(\"Response:\", encoded_train[0][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af45a25",
   "metadata": {},
   "source": [
    "##Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "997d2faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset (Dataset):\n",
    "    def __init__(self,pair):\n",
    "        self.pair = pair\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pair)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.pair[index]\n",
    "    \n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_seqs, target_seqs = zip(*batch)\n",
    "\n",
    "    input_seqs = [torch.tensor(seq, dtype=torch.long) for seq in input_seqs]\n",
    "\n",
    "    tgt_inputs = []\n",
    "    tgt_outputs = []\n",
    "    for tgt in target_seqs:\n",
    "        tgt_input = tgt[:-1]  # all except <eos>\n",
    "        tgt_output = tgt[1:]  # all except <sos>\n",
    "        tgt_inputs.append(torch.tensor(tgt_input, dtype=torch.long))\n",
    "        tgt_outputs.append(torch.tensor(tgt_output, dtype=torch.long))\n",
    "\n",
    "    input_padded = pad_sequence(input_seqs, batch_first=True, padding_value=word2idx['<pad>'])\n",
    "    tgt_input_padded = pad_sequence(tgt_inputs, batch_first=True, padding_value=word2idx['<pad>'])\n",
    "    tgt_output_padded = pad_sequence(tgt_outputs, batch_first=True, padding_value=word2idx['<pad>'])\n",
    "\n",
    "    return input_padded, tgt_input_padded, tgt_output_padded\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63785d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = ChatDataset(encoded_train)\n",
    "val_dataset   = ChatDataset(encoded_val)\n",
    "test_dataset  = ChatDataset(encoded_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e867661",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "368dfe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6aa00620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerChatbot(nn.Module):\n",
    "    def __init__(self, fasttext_embeddings ,vocab_size,pad_idx ,emb_dim = 300, nhead = 6, num_layers = 3, dim_ff = 512, dropout =0.1):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(fasttext_embeddings, freeze=False)\n",
    "        self.pos_encoder = PositionalEncoding(emb_dim)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=emb_dim,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(emb_dim, vocab_size)\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # Padding masks\n",
    "        src_key_padding_mask = (src == self.pad_idx)\n",
    "        tgt_key_padding_mask = (tgt == self.pad_idx)\n",
    "\n",
    "        # Causal mask for decoder\n",
    "        tgt_seq_len = tgt.size(1)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_seq_len).to(src.device)\n",
    "\n",
    "        # Embedding + Positional Encoding\n",
    "        src_emb = self.pos_encoder(self.embedding_layer(src))\n",
    "        tgt_emb = self.pos_encoder(self.embedding_layer(tgt))\n",
    "\n",
    "        # Transformer\n",
    "        output = self.transformer(\n",
    "            src=src_emb,\n",
    "            tgt=tgt_emb,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            tgt_mask=tgt_mask\n",
    "        )\n",
    "\n",
    "        return self.fc_out(output)  # shape [batch, tgt_len, vocab_size]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc5f62c",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f4dc77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model = TransformerChatbot(\n",
    "    vocab_size=len(word2idx),\n",
    "    emb_dim=300,\n",
    "    nhead=6,\n",
    "    num_layers=3,\n",
    "    dim_ff=512,\n",
    "    dropout=0.1,\n",
    "    pad_idx=word2idx['<pad>'],\n",
    "    fasttext_embeddings=fasttext_embeddings\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf67eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = word2idx['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2b7b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for src, tgt_inp, tgt_out in loader:\n",
    "        src, tgt_inp, tgt_out = src.to(device), tgt_inp.to(device), tgt_out.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt_inp)  # [B, T, V]\n",
    "        loss = criterion(output.view(-1, output.size(-1)), tgt_out.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for src, tgt_inp, tgt_out in loader:\n",
    "        src, tgt_inp, tgt_out = src.to(device), tgt_inp.to(device), tgt_out.to(device)\n",
    "        output = model(src, tgt_inp)\n",
    "        loss = criterion(output.view(-1, output.size(-1)), tgt_out.view(-1))\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for src, tgt_inp, tgt_out in loader:\n",
    "        src, tgt_inp, tgt_out = src.to(device), tgt_inp.to(device), tgt_out.to(device)\n",
    "        output = model(src, tgt_inp)\n",
    "        loss = criterion(output.view(-1, output.size(-1)), tgt_out.view(-1))\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6957a0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\chatbot\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss = evaluate(model, val_loader, criterion)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a11e290",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = test(model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834a5820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, input_sentence, word2idx, idx2word, max_len=50):\n",
    "    model.eval()\n",
    "\n",
    "    # 1. Tokenize and numericalize input\n",
    "    tokens = input_sentence.strip().split()  # Or use your tokenizer\n",
    "    input_ids = [word2idx.get(tok, word2idx['<unk>']) for tok in tokens]\n",
    "    src = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "\n",
    "    # 2. Start with <sos> token for decoder input\n",
    "    generated_ids = [word2idx['<sos>']]\n",
    "    for _ in range(max_len):\n",
    "        tgt = torch.tensor([generated_ids], dtype=torch.long).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            output = model(src, tgt)  # [1, T, V]\n",
    "            next_token_logits = output[0, -1, :]  # last token's logits\n",
    "            next_token_id = torch.argmax(next_token_logits).item()\n",
    "\n",
    "        # Stop if <eos>\n",
    "        if next_token_id == word2idx['<eos>']:\n",
    "            break\n",
    "\n",
    "        generated_ids.append(next_token_id)\n",
    "\n",
    "    # 3. Decode token ids to words\n",
    "    generated_words = [idx2word[idx] for idx in generated_ids[1:]]  # remove <sos>\n",
    "    return ' '.join(generated_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b7a83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input(\"Input: \")\n",
    "    if user_input.lower() in ['exit', 'quit']:\n",
    "        break\n",
    "    response = generate_response(model, user_input, word2idx, idx2word)\n",
    "    print(\"Response:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
