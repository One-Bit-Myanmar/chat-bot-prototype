{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5619be5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211c8db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, embedding, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.lstm = nn.LSTM(embedding.embedding_dim, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: [batch_size, src_len]\n",
    "        embedded = self.embedding(src)  # [batch_size, src_len, emb_dim]\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return hidden, cell\n",
    "    \n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, embedding, hidden_size, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.lstm = nn.LSTM(embedding.embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, trg, hidden, cell):\n",
    "        # trg: [batch_size, trg_len]\n",
    "        embedded = self.embedding(trg)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        logits = self.fc_out(outputs)  # [batch_size, trg_len, output_dim]\n",
    "        return logits, hidden, cell\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        hidden, cell = self.encoder(src)\n",
    "        outputs, _, _ = self.decoder(trg, hidden, cell)\n",
    "        return outputs  # [batch_size, trg_len, vocab_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dccd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
    "\n",
    "def load_text_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def split_into_pairs(text):\n",
    "    utterances = [utt.strip() for utt in text.split('__eou__') if utt.strip()]\n",
    "    pairs = [(utterances[i], utterances[i + 1]) for i in range(len(utterances) - 1)]\n",
    "    return pairs\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s']\", \"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def build_vocab(pairs, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for inp, res in pairs:\n",
    "        counter.update(preprocess_text(inp).split())\n",
    "        counter.update(preprocess_text(res).split())\n",
    "    vocab = SPECIAL_TOKENS.copy()\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq and word not in vocab:\n",
    "            vocab.append(word)\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    return word2idx, idx2word\n",
    "\n",
    "def tokenize(text, word2idx):\n",
    "    tokens = preprocess_text(text).split()\n",
    "    return [word2idx.get(token, word2idx['<unk>']) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682255be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self, pairs, word2idx):\n",
    "        self.pairs = pairs\n",
    "        self.word2idx = word2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp, res = self.pairs[idx]\n",
    "        inp_ids = tokenize(inp, self.word2idx)\n",
    "        res_input = [self.word2idx['<sos>']] + tokenize(res, self.word2idx)\n",
    "        res_target = tokenize(res, self.word2idx) + [self.word2idx['<eos>']]\n",
    "        return torch.tensor(inp_ids), torch.tensor(res_input), torch.tensor(res_target)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, res_inputs, res_targets = zip(*batch)\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=word2idx['<pad>'])\n",
    "    res_inputs_padded = pad_sequence(res_inputs, batch_first=True, padding_value=word2idx['<pad>'])\n",
    "    res_targets_padded = pad_sequence(res_targets, batch_first=True, padding_value=word2idx['<pad>'])\n",
    "    return inputs_padded, res_inputs_padded, res_targets_padded\n",
    "\n",
    "# ----------- USAGE ------------\n",
    "\n",
    "file_path = 'dataset/dialogues_train.txt'  # put your filename here\n",
    "raw_text = load_text_from_file(file_path)\n",
    "pairs = split_into_pairs(raw_text)\n",
    "word2idx, idx2word = build_vocab(pairs, min_freq=2)\n",
    "\n",
    "dataset = ChatbotDataset(pairs, word2idx)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Example: fetch one batch\n",
    "for batch_inp, batch_res_inp, batch_res_tgt in dataloader:\n",
    "    print(batch_inp.shape)       # (batch_size, seq_len)\n",
    "    print(batch_res_inp.shape)   # (batch_size, seq_len)\n",
    "    print(batch_res_tgt.shape)   # (batch_size, seq_len)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ddfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "hidden_size = 512\n",
    "model = Seq2Seq(encoder=EncoderLSTM(embed_size, hidden_size),\n",
    "                decoder=DecoderLSTM(embed_size, hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf0cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "for epoch in range(30):\n",
    "    for input, target in train_data:\n",
    "       output = model(input)  \n",
    "       loss = criterion(output, target)\n",
    "       loss.backward()  \n",
    "       optimizer.step()\n",
    "       optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638e06a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469d60f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "scores = evaluate(model, val_data, tokenizer)  \n",
    "print(f\"Perplexity score: {scores['perplexity']}\")\n",
    "print(f\"BLEU score: {scores['bleu']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
