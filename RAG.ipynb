{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d61f51",
   "metadata": {},
   "source": [
    "This code is from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aa871dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'\n",
    "LANGUAGE_MODEL = 'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF'\n",
    "\n",
    "# Each element in the VECTOR_DB will be a tuple (chunk, embedding)\n",
    "# The embedding is a list of floats, for example: [0.1, 0.04, -0.34, 0.21, ...]\n",
    "VECTOR_DB = []\n",
    "\n",
    "def add_chunk_to_database(chunk):\n",
    "  embedding = ollama.embed(model=EMBEDDING_MODEL, input=chunk)['embeddings'][0]\n",
    "  VECTOR_DB.append((chunk, embedding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23aea603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7 entries\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "with open('test.txt', 'r',encoding=\"utf-8\") as file:\n",
    "  dataset = file.readlines()\n",
    "  print(f'Loaded {len(dataset)} entries')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3851251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk 1/7 to the database\n",
      "Added chunk 2/7 to the database\n",
      "Added chunk 3/7 to the database\n",
      "Added chunk 4/7 to the database\n",
      "Added chunk 5/7 to the database\n",
      "Added chunk 6/7 to the database\n",
      "Added chunk 7/7 to the database\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(dataset):\n",
    "  add_chunk_to_database(chunk)\n",
    "  print(f'Added chunk {i+1}/{len(dataset)} to the database')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a392a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "  dot_product = sum([x * y for x, y in zip(a, b)])\n",
    "  norm_a = sum([x ** 2 for x in a]) ** 0.5\n",
    "  norm_b = sum([x ** 2 for x in b]) ** 0.5\n",
    "  return dot_product / (norm_a * norm_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94e1ad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_n=3):\n",
    "  query_embedding = ollama.embed(model=EMBEDDING_MODEL, input=query)['embeddings'][0]\n",
    "  # temporary list to store (chunk, similarity) pairs\n",
    "  similarities = []\n",
    "  for chunk, embedding in VECTOR_DB:\n",
    "    similarity = cosine_similarity(query_embedding, embedding)\n",
    "    similarities.append((chunk, similarity))\n",
    "  # sort by similarity in descending order, because higher similarity means more relevant chunks\n",
    "  similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "  # finally, return the top N most relevant chunks\n",
    "  return similarities[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32a44ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved knowledge:\n",
      " - (similarity: 0.58) I can ’ t believe it ! __eou__ What ’ s wrong ? That was a great goal . __eou__ Yes , but I bet $ 200 dollars on the Cougars ! __eou__ Looks like you ’ re going to lose out on this game then . __eou__ I can ’ t believe it ! I thought the Cougars were going to win for sure . __eou__ What were the odds ? __eou__ 20 to 1 , in favour of the Cougars ! __eou__ Too bad . __eou__\n",
      "\n",
      " - (similarity: 0.58) Merry Christmas , Linda ! __eou__ Merry Christmas , Lee ! __eou__ Linda , thank you for this wonderful Christmas dinner . I really enjoyed it . __eou__ Thank you for coming . I'm glad you liked it . __eou__\n",
      "\n",
      " - (similarity: 0.57) Merry Christmas , Bill ! __eou__ Merry Christmas . Steven ! __eou__ What do you want to do to celebrate Christmas ? __eou__ I'll have supper with my girlfriend and go shopping . What about you ? __eou__ I'll go to church first and then go to a Christmas party . Would you like to come ? __eou__ I'd love to . But I am going to meet my girlfriend right now . See you later . __eou__ See you . __eou__\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_query = input('Ask me a question: ')\n",
    "retrieved_knowledge = retrieve(input_query)\n",
    "\n",
    "print('Retrieved knowledge:')\n",
    "for chunk, similarity in retrieved_knowledge:\n",
    "  print(f' - (similarity: {similarity:.2f}) {chunk}')\n",
    "\n",
    "instruction_prompt = f'''You are a helpful chatbot.\n",
    "Use only the following pieces of context to answer the question. Don't make up any new information:\n",
    "{'\\n'.join([f' - {chunk}' for chunk, similarity in retrieved_knowledge])}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f8dcf5",
   "metadata": {},
   "source": [
    "```\n",
    "    ollama pull hf.co/CompendiumLabs/bge-base-en-v1.5-gguf\n",
    "    ollama pull hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
